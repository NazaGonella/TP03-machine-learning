{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692055db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.data_handler as data_handler\n",
    "import src.models as models\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "CANTIDAD_DE_CLASES = 48\n",
    "SEED = 42\n",
    "\n",
    "X_images : np.ndarray[float] = np.load(f\"{project_root}/TP03/data/X_images.npy\")\n",
    "y_images : np.ndarray[float] = np.load(f\"{project_root}/TP03/data/y_images.npy\")\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b03d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_images = np.array([[0 if y_images[x] != i else 1 for i in range(CANTIDAD_DE_CLASES)] for x in range(len(X_images))], dtype=float)\n",
    "X_images = X_images / 255\n",
    "X_train : pd.DataFrame\n",
    "X_validation : pd.DataFrame\n",
    "X_test : pd.DataFrame\n",
    "X_train, X_validation, X_test, Y_train, Y_validation, Y_test = data_handler.get_splitted_dataset(pd.DataFrame(X_images), pd.DataFrame(y_images), seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94b6e6",
   "metadata": {},
   "source": [
    "## 3 ) Implementación y Entrenamiento de una Red Neuronal Avanzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44625781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo diferentes batch sizes\n",
    "# for i in range(4, 11, 1):\n",
    "#     print(f\"\\nBATCH SIZE = 2^{i}\")\n",
    "#     M1_batch_sizes : models.RedNeuronal = models.RedNeuronal([100,80], ['relu', 'relu', 'softmax'])\n",
    "#     M1_batch_sizes.stochastic_gradient_descent(\n",
    "#         np.array(X_train), \n",
    "#         np.array(Y_train),\n",
    "#         epochs=250, \n",
    "#         learning_rate=[0.0001, 0.0001], \n",
    "#         batch_size_2_pow=i, \n",
    "#         print_results_rate=125\n",
    "#     )\n",
    "\n",
    "# output\n",
    "\n",
    "# BATCH SIZE = 2^4\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.4172952929689673\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 3.872721248866749\n",
    "# -> Difference = +2.4554259558977813\n",
    "\n",
    "# BATCH SIZE = 2^5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.068959884185001\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 3.852873102551695\n",
    "# -> Difference = +1.7839132183666937\n",
    "\n",
    "# BATCH SIZE = 2^6\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.503137802246227\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 3.8621948632425784\n",
    "# -> Difference = +2.3590570609963515\n",
    "\n",
    "# BATCH SIZE = 2^7\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1309850462552273\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 3.8645668214647095\n",
    "# -> Difference = +2.7335817752094824\n",
    "\n",
    "# BATCH SIZE = 2^8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.5913105224784112\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 4.343482088518529\n",
    "# -> Difference = +2.752171566040118\n",
    "\n",
    "# BATCH SIZE = 2^9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.237209996813813\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 3.124517962619984\n",
    "# -> Difference = +1.887307965806171\n",
    "\n",
    "# BATCH SIZE = 2^10\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1415337620477084\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9448982327931914\n",
    "# -> Difference = -0.196635529254517\n",
    "\n",
    "\n",
    "# Mejores valores:\n",
    "# -> 7\n",
    "# -> 8\n",
    "# -> 9\n",
    "# -> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7deb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo diferentes K (Rate Schedule Lineal)\n",
    "# for i in np.arange(1, 8.5, 0.5):\n",
    "#     print(f\"\\nK = epochs / {i} = {int(round(250/i))}\")\n",
    "#     M1_rs_lineal : models.RedNeuronal = models.RedNeuronal([100,80], ['relu', 'relu', 'softmax'])\n",
    "#     M1_rs_lineal.stochastic_gradient_descent(\n",
    "#         np.array(X_train), \n",
    "#         np.array(Y_train),\n",
    "#         epochs=250, \n",
    "#         learning_rate=[0.0001, 0.00001], \n",
    "#         batch_size_2_pow=10, \n",
    "#         K=int(round(250/i)),\n",
    "#         print_results_rate=125\n",
    "#     )\n",
    "\n",
    "# outputs\n",
    "\n",
    "# K = epochs / 1.0 = 250\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2315667779944102\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9282257182237148\n",
    "# -> Difference = -0.30334105977069536\n",
    "\n",
    "# K = epochs / 1.5 = 167\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.3457219686135664\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.18082896379247\n",
    "# -> Difference = -0.16489300482109637\n",
    "\n",
    "# K = epochs / 2.0 = 125\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.5599001239326573\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.402887814776599\n",
    "# -> Difference = -0.15701230915605824\n",
    "\n",
    "# K = epochs / 2.5 = 100\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.731865305547131\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.5258190848871978\n",
    "# -> Difference = -0.20604622065993317\n",
    "\n",
    "# K = epochs / 3.0 = 83\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.8668644240816181\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.6246906793329419\n",
    "# -> Difference = -0.24217374474867626\n",
    "\n",
    "# K = epochs / 3.5 = 71\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.028664632482819\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.7096246003050268\n",
    "# -> Difference = -0.3190400321777922\n",
    "\n",
    "# K = epochs / 4.0 = 62\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.193718177981163\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.8334227580855653\n",
    "# -> Difference = -0.3602954198955979\n",
    "\n",
    "# K = epochs / 4.5 = 56\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.3154135465234127\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.882055754810544\n",
    "# -> Difference = -0.4333577917128686\n",
    "\n",
    "# K = epochs / 5.0 = 50\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.4534161910034094\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.997947726239879\n",
    "# -> Difference = -0.4554684647635303\n",
    "\n",
    "# K = epochs / 5.5 = 45\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.4208210545797697\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.9626379885146958\n",
    "# -> Difference = -0.45818306606507386\n",
    "\n",
    "# K = epochs / 6.0 = 42\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.5536363593431286\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 2.0566346996991447\n",
    "# -> Difference = -0.4970016596439839\n",
    "\n",
    "# K = epochs / 6.5 = 38\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.4199395417680583\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.982755603538604\n",
    "# -> Difference = -0.4371839382294542\n",
    "\n",
    "# K = epochs / 7.0 = 36\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.591578743678074\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 2.0582109992993223\n",
    "# -> Difference = -0.5333677443787517\n",
    "\n",
    "# K = epochs / 7.5 = 33\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.8145412582937697\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 2.2594012716020897\n",
    "# -> Difference = -0.55513998669168\n",
    "\n",
    "# K = epochs / 8.0 = 31\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.7476434093148048\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 2.1596900099530254\n",
    "# -> Difference = -0.5879533993617794\n",
    "\n",
    "\n",
    "# Mejores Valores:\n",
    "# -> K = epochs / 1.0\n",
    "# -> K = epochs / 1.5\n",
    "# -> K = epochs / 7.5\n",
    "# -> K = epochs / 8.0\n",
    "# La diferencia de Loss entre épocas es mayor a medida se aumenta K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4488301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo diferentes S (Rate Schedule Exponencial)\n",
    "# con c = -0.5\n",
    "# for i in np.arange(1, 8.5, 0.5):\n",
    "#     print(f\"\\nS = epochs / {i}\")\n",
    "#     M1_rs_lineal : models.RedNeuronal = models.RedNeuronal([100,80], ['relu', 'relu', 'softmax'])\n",
    "#     M1_rs_lineal.stochastic_gradient_descent(\n",
    "#         np.array(X_train), \n",
    "#         np.array(Y_train),\n",
    "#         epochs=250, \n",
    "#         learning_rate=[0.0001, 0.00001], \n",
    "#         batch_size_2_pow=10, \n",
    "#         S = 250 / i,\n",
    "#         c = -0.5,\n",
    "#         print_results_rate=125\n",
    "#     )\n",
    "\n",
    "# outputs\n",
    "\n",
    "# S = epochs / 1.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2315667779944102\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9282257182237149\n",
    "# -> Difference = -0.30334105977069525\n",
    "\n",
    "# S = epochs / 1.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2279800986162261\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.934442884522318\n",
    "# -> Difference = -0.2935372140939081\n",
    "\n",
    "# S = epochs / 2.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.27159213924753\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9733160030805783\n",
    "# -> Difference = -0.2982761361669517\n",
    "\n",
    "# S = epochs / 2.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2561138691269322\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9592860523539999\n",
    "# -> Difference = -0.29682781677293235\n",
    "\n",
    "# S = epochs / 3.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.254764903238054\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9703015449309709\n",
    "# -> Difference = -0.284463358307083\n",
    "\n",
    "# S = epochs / 3.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2392888075717492\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9572990855764858\n",
    "# -> Difference = -0.2819897219952634\n",
    "\n",
    "# S = epochs / 4.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2396458694007957\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9494928606444151\n",
    "# -> Difference = -0.2901530087563806\n",
    "\n",
    "# S = epochs / 4.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2182932524777628\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9143984963273055\n",
    "# -> Difference = -0.30389475615045725\n",
    "\n",
    "# S = epochs / 5.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2660397495247033\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9607409005321466\n",
    "# -> Difference = -0.30529884899255677\n",
    "\n",
    "# S = epochs / 5.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.231261411530577\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9455057707748208\n",
    "# -> Difference = -0.2857556407557563\n",
    "\n",
    "# S = epochs / 6.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2662249524660232\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9660696978410621\n",
    "# -> Difference = -0.30015525462496107\n",
    "\n",
    "# S = epochs / 6.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2266666745874328\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9417029115538114\n",
    "# -> Difference = -0.2849637630336215\n",
    "\n",
    "# S = epochs / 7.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2318325649500805\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9540861023732001\n",
    "# -> Difference = -0.2777464625768804\n",
    "\n",
    "# S = epochs / 7.5\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.23548550539208\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9268702820964739\n",
    "# -> Difference = -0.308615223295606\n",
    "\n",
    "# S = epochs / 8.0\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2184017920804486\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9384180833074381\n",
    "# -> Difference = -0.2799837087730105\n",
    "\n",
    "\n",
    "# Mejores valores:\n",
    "# -> S = epochs / 1.0\n",
    "# -> S = epochs / 4.5\n",
    "# -> S = epochs / 8.0\n",
    "# -> S = epochs / 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b23763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo diferentes combinaciones de B1 y B2 (Adam)\n",
    "# valores comunes para parámetros de Adam:\n",
    "# b1 = [0.85, 0.9, 0.95]\n",
    "# b2 = [0.95, 0.99, 0.999]\n",
    "# for b1 in [0.8, 0.85, 0.9, 0.95, 0.99, 0.999]:\n",
    "#     for b2 in [0.8, 0.85, 0.9, 0.95, 0.99, 0.999]:\n",
    "#         print(f\"\\nB1 = {b1}\")\n",
    "#         print(f\"B2 = {b2}\")\n",
    "#         M1_rs_lineal : models.RedNeuronal = models.RedNeuronal([100,80], ['relu', 'relu', 'softmax'])\n",
    "#         M1_rs_lineal.stochastic_gradient_descent(\n",
    "#             np.array(X_train), \n",
    "#             np.array(Y_train),\n",
    "#             epochs=250, \n",
    "#             learning_rate=[0.0001, 0.0001], \n",
    "#             batch_size_2_pow=10, \n",
    "#             use_adam=True,\n",
    "#             b1=b1,\n",
    "#             b2=b2,\n",
    "#             print_results_rate=125\n",
    "#         )\n",
    "\n",
    "# B1 = 0.8\n",
    "# B2 = 0.8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7772490205564546\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9639208596747603\n",
    "# -> Difference = -0.8133281608816944\n",
    "\n",
    "# B1 = 0.8\n",
    "# B2 = 0.85\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7983328000649013\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9838414169872784\n",
    "# -> Difference = -0.8144913830776228\n",
    "\n",
    "# B1 = 0.8\n",
    "# B2 = 0.9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7607794740484755\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9893162812603814\n",
    "# -> Difference = -0.771463192788094\n",
    "\n",
    "# B1 = 0.8\n",
    "# B2 = 0.95\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.8052086836089671\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.001376278851238\n",
    "# -> Difference = -0.8038324047577292\n",
    "\n",
    "# B1 = 0.8\n",
    "# B2 = 0.99\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.9510067638275015\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1222462163537101\n",
    "# -> Difference = -0.8287605474737914\n",
    "\n",
    "# B1 = 0.8\n",
    "# B2 = 0.999\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.162745869801885\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.3225639985527846\n",
    "# -> Difference = -0.8401818712491003\n",
    "\n",
    "# B1 = 0.85\n",
    "# B2 = 0.8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7999609420546678\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9977574691931932\n",
    "# -> Difference = -0.8022034728614746\n",
    "\n",
    "# B1 = 0.85\n",
    "# B2 = 0.85\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.798068963360069\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9548216146609436\n",
    "# -> Difference = -0.8432473486991254\n",
    "\n",
    "# B1 = 0.85\n",
    "# B2 = 0.9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.8159731602463947\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.0158137090153114\n",
    "# -> Difference = -0.8001594512310832\n",
    "\n",
    "# B1 = 0.85\n",
    "# B2 = 0.95\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.8163240591156158\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.0149400954495191\n",
    "# -> Difference = -0.8013839636660967\n",
    "\n",
    "# B1 = 0.85\n",
    "# B2 = 0.99\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.000600466751602\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1251001938259066\n",
    "# -> Difference = -0.8755002729256955\n",
    "\n",
    "# B1 = 0.85\n",
    "# B2 = 0.999\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.078086135678435\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.3342131161697164\n",
    "# -> Difference = -0.7438730195087186\n",
    "\n",
    "# B1 = 0.9\n",
    "# B2 = 0.8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7053035881717984\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9601541105674611\n",
    "# -> Difference = -0.7451494776043373\n",
    "\n",
    "# B1 = 0.9\n",
    "# B2 = 0.85\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7979887343348615\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9719501225201559\n",
    "# -> Difference = -0.8260386118147056\n",
    "\n",
    "# B1 = 0.9\n",
    "# B2 = 0.9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.747080045171262\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9923929049795421\n",
    "# -> Difference = -0.7546871401917199\n",
    "\n",
    "# B1 = 0.9\n",
    "# B2 = 0.95\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7713394597958545\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9932174300782322\n",
    "# -> Difference = -0.7781220297176223\n",
    "\n",
    "# B1 = 0.9\n",
    "# B2 = 0.99\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.9661058428118556\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1222052547226222\n",
    "# -> Difference = -0.8439005880892334\n",
    "\n",
    "# B1 = 0.9\n",
    "# B2 = 0.999\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.1198485219230068\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.3143973380613372\n",
    "# -> Difference = -0.8054511838616696\n",
    "\n",
    "# B1 = 0.95\n",
    "# B2 = 0.8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7359165937813512\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9822689844351626\n",
    "# -> Difference = -0.7536476093461886\n",
    "\n",
    "# B1 = 0.95\n",
    "# B2 = 0.85\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7439409260045875\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.0122078843606628\n",
    "# -> Difference = -0.7317330416439247\n",
    "\n",
    "# B1 = 0.95\n",
    "# B2 = 0.9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7420183818955586\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9713205953858455\n",
    "# -> Difference = -0.7706977865097131\n",
    "\n",
    "# B1 = 0.95\n",
    "# B2 = 0.95\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.713992644969066\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.955531323085762\n",
    "# -> Difference = -0.7584613218833038\n",
    "\n",
    "# B1 = 0.95\n",
    "# B2 = 0.99\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.9823285772161083\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1141049453740333\n",
    "# -> Difference = -0.8682236318420751\n",
    "\n",
    "# B1 = 0.95\n",
    "# B2 = 0.999\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.0584884572132047\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.3331926942923464\n",
    "# -> Difference = -0.7252957629208583\n",
    "\n",
    "# B1 = 0.99\n",
    "# B2 = 0.8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.9194829327123957\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 22.12640362799161\n",
    "# -> Difference = +20.206920695279216\n",
    "\n",
    "# B1 = 0.99\n",
    "# B2 = 0.85\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.753760525407771\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1444816278394123\n",
    "# -> Difference = -0.6092788975683587\n",
    "\n",
    "# B1 = 0.99\n",
    "# B2 = 0.9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.7613764139245738\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1592166088031157\n",
    "# -> Difference = -0.6021598051214581\n",
    "\n",
    "# B1 = 0.99\n",
    "# B2 = 0.95\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.5711968091365154\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9890369904033487\n",
    "# -> Difference = -0.5821598187331667\n",
    "\n",
    "# B1 = 0.99\n",
    "# B2 = 0.99\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.6961132465457276\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9941542919174201\n",
    "# -> Difference = -0.7019589546283075\n",
    "\n",
    "# B1 = 0.99\n",
    "# B2 = 0.999\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.827154957442074\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.2084058959905646\n",
    "# -> Difference = -0.6187490614515094\n",
    "\n",
    "# B1 = 0.999\n",
    "# B2 = 0.8\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 22.009892572040744\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 22.058945080345595\n",
    "# -> Difference = +0.049052508304850306\n",
    "\n",
    "# B1 = 0.999\n",
    "# B2 = 0.85\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.067561520696059\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 6.878628816964356\n",
    "# -> Difference = +4.8110672962682965\n",
    "\n",
    "# B1 = 0.999\n",
    "# B2 = 0.9\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 2.0831593557943204\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 2.5461054926145916\n",
    "# -> Difference = +0.4629461368202712\n",
    "\n",
    "# B1 = 0.999\n",
    "# B2 = 0.95\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.800115137420336\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.4678725883521437\n",
    "# -> Difference = -0.3322425490681922\n",
    "\n",
    "# B1 = 0.999\n",
    "# B2 = 0.99\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.6069005465455553\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9377605121728853\n",
    "# -> Difference = -0.66914003437267\n",
    "\n",
    "# B1 = 0.999\n",
    "# B2 = 0.999\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.6909541426207872\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.0714718472439175\n",
    "# -> Difference = -0.6194822953768697\n",
    "\n",
    "# Mejores valores:\n",
    "# -> B1 = 0.999\n",
    "#    B2 = 0.99\n",
    "# -> B1 = 0.85\n",
    "#    B2 = 0.85\n",
    "# -> B1 = 0.95\n",
    "#    B2 = 0.95\n",
    "# -> B1 = 0.9\n",
    "#    B2 = 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47874b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo diferentes L2 (Regularización L2)\n",
    "# for l2_exp in range(3, -6, -1):\n",
    "#     print(f\"\\nL2 = {10**l2_exp}\")\n",
    "#     M1 : models.RedNeuronal = models.RedNeuronal([100,80], ['relu', 'relu', 'softmax'])\n",
    "#     M1.stochastic_gradient_descent(\n",
    "#         np.array(X_train), \n",
    "#         np.array(Y_train),\n",
    "#         epochs=250, \n",
    "#         learning_rate=[0.0001, 0.0001], \n",
    "#         batch_size_2_pow=10, \n",
    "#         L2=10**l2_exp,\n",
    "#         print_results_rate=125\n",
    "#     )\n",
    "\n",
    "# output:\n",
    "\n",
    "# L2 = 1000\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 3.529896700969622\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 3.154754735208252\n",
    "# -> Difference = -0.3751419657613697\n",
    "\n",
    "# L2 = 100\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.8320888855557524\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.4721796749388958\n",
    "# -> Difference = -0.35990921061685666\n",
    "\n",
    "# L2 = 10\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.2211964751367268\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.9547285923058941\n",
    "# -> Difference = -0.26646788283083267\n",
    "\n",
    "# L2 = 1\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1679356882332232\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.8247530396746727\n",
    "# -> Difference = -0.3431826485585505\n",
    "\n",
    "# L2 = 0.1\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1220241383402538\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.36359759909518\n",
    "# -> Difference = +0.24157346075492625\n",
    "\n",
    "# L2 = 0.01\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1006828117873595\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.1026206058737746\n",
    "# -> Difference = +0.0019377940864151455\n",
    "\n",
    "# L2 = 0.001\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1423622218425207\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.2426144683853708\n",
    "# -> Difference = +0.10025224654285014\n",
    "\n",
    "# L2 = 0.0001\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.060911088368837\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 1.0485479311999377\n",
    "# -> Difference = -0.012363157168899352\n",
    "\n",
    "# L2 = 1e-05\n",
    "# Epoch 125\n",
    "# -> Loss Mean = 1.1213377292808744\n",
    "# Epoch 250\n",
    "# -> Loss Mean = 0.8520883823783536\n",
    "# -> Difference = -0.2692493469025208\n",
    "\n",
    "\n",
    "# Mejores valores:\n",
    "# -> 0.00001\n",
    "# -> 0.0001\n",
    "# -> 1\n",
    "# -> 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d20664",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eeb6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de iteraciones estimada: 64000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (128,48) (128,100) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     12\u001b[39m L2_values : \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] = [\u001b[32m0.00001\u001b[39m, \u001b[32m0.0001\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m]\n\u001b[32m     13\u001b[39m cross_validation : models.CrossValidation = models.CrossValidation(\n\u001b[32m     14\u001b[39m     np.array(X_train),\n\u001b[32m     15\u001b[39m     np.array(Y_train),\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     L2_values=L2_values,\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_hiperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msoftmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UDESA/5toCUATRIMESTRE/ML/TPs/TP03/src/models.py:68\u001b[39m, in \u001b[36mCrossValidation.evaluate_hiperparameters\u001b[39m\u001b[34m(self, M, h)\u001b[39m\n\u001b[32m     65\u001b[39m Y_train, Y_val = \u001b[38;5;28mself\u001b[39m.Y[train_indices], \u001b[38;5;28mself\u001b[39m.Y[val_indices]\n\u001b[32m     67\u001b[39m model : RedNeuronal = RedNeuronal(M, h)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstochastic_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size_2_pow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[43m=\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_adam\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_adam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mb1_b2\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mb1_b2\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mL2\u001b[49m\u001b[43m=\u001b[49m\u001b[43ml2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m preds = model.forward_pass(X_val, model.W, model.w_0)\n\u001b[32m     83\u001b[39m loss = model.cross_entropy(Y_val, preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UDESA/5toCUATRIMESTRE/ML/TPs/TP03/src/models.py:276\u001b[39m, in \u001b[36mRedNeuronal.stochastic_gradient_descent\u001b[39m\u001b[34m(self, X, Y, epochs, learning_rate, batch_size_2_pow, K, c, S, use_adam, b1, b2, L2, print_results_rate)\u001b[39m\n\u001b[32m    274\u001b[39m X_b : np.ndarray = X[batch * batch_size : (batch + \u001b[32m1\u001b[39m) * batch_size]\n\u001b[32m    275\u001b[39m Y_b : np.ndarray = Y[batch * batch_size : (batch + \u001b[32m1\u001b[39m) * batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m pred, loss, dW, dw_0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomputar_gradiente\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mL2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# actualizaciones\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.L - \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UDESA/5toCUATRIMESTRE/ML/TPs/TP03/src/models.py:180\u001b[39m, in \u001b[36mRedNeuronal.computar_gradiente\u001b[39m\u001b[34m(self, x, y, W, w_0, L2)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcomputar_gradiente\u001b[39m(\u001b[38;5;28mself\u001b[39m, x : np.ndarray, y : np.ndarray, W : np.ndarray, w_0 : np.ndarray, L2 : \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m) -> np.ndarray:\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    179\u001b[39m     pred : np.ndarray = \u001b[38;5;28mself\u001b[39m.forward_pass(x, W, w_0)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     loss : np.ndarray = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# Backward-pass\u001b[39;00m\n\u001b[32m    182\u001b[39m     dW : np.ndarray\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/UDESA/5toCUATRIMESTRE/ML/TPs/TP03/src/models.py:135\u001b[39m, in \u001b[36mRedNeuronal.cross_entropy\u001b[39m\u001b[34m(self, y_true, y_pred)\u001b[39m\n\u001b[32m    133\u001b[39m y_pred = np.clip(y_pred, epsilon, \u001b[32m1\u001b[39m - epsilon)\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# return - (y_true.T @ np.log(y_pred))\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -np.sum(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (128,48) (128,100) "
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_handler.get_train_and_test_split(pd.DataFrame(X_images), pd.DataFrame(y_images), seed=SEED)\n",
    "\n",
    "# HIPERPARÁMETROS\n",
    "# Valores candidatos:\n",
    "epochs : int = 250\n",
    "lr_range_values : list[tuple[float, float]] = [(0.0001, 0.0001), (0.01, 0.001), (0.001, 0.0001), (0.0001, 0.00001)]\n",
    "batch_size_2_pow_values : list[int] = [7, 8, 9, 10]\n",
    "K_values : list[int] = [0, int(round(epochs/1.0)), int(round(epochs/1.5)), int(round(epochs/7.5)), int(round(epochs/8.0))]\n",
    "c_values : list[float] = [0, -0.5, -1, -2, -4]\n",
    "S_values : list[float] =  [0, epochs / 1.0, epochs / 4.5, epochs / 8.0, epochs / 7.5]\n",
    "b1_and_b2_values : list[float] =  [(0.999, 0.99), (0.85, 0.85), (0.95, 0.95), (0.9, 0.88)]\n",
    "L2_values : list[float] = [0.00001, 0.0001, 1, 10]\n",
    "cross_validation : models.CrossValidation = models.CrossValidation(\n",
    "    np.array(X_train),\n",
    "    np.array(Y_train),\n",
    "    epochs=epochs,\n",
    "    num_folds=4,\n",
    "    learning_rate_values=lr_range_values,\n",
    "    batch_size_2_pow_values=batch_size_2_pow_values,\n",
    "    K_values=K_values,\n",
    "    c_values=c_values,\n",
    "    S_values=S_values,\n",
    "    b1_and_b2_values=b1_and_b2_values,\n",
    "    L2_values=L2_values,\n",
    ")\n",
    "cross_validation.evaluate_hiperparameters(M=[100,80], h=['relu', 'relu', 'softmax'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
